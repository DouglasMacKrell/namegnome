# namegnome v2 — TASK_6-26-25.md

> **Prompt to AI:** "Update TASK_6-26-25.md to mark XYZ as done and add ABC as a new task."

---

## Test-Driven Development (TDD) Policy

All work defined in this document **must** follow TDD:

* **Write or update a failing test before fixing/adding code.**
* **Only write the minimum code to make the test pass.**
* **Refactor with tests green.**
* **Document test coverage and rationale for each ticket.**
* **No ticket is complete until all acceptance criteria are met and tests pass.**

---

## 1 · Sprint 0 (Baseline & CLI Polish)

Purpose: establish a solid CLI UX foundation and ensure every flag/option is discoverable and covered by tests.

### 0.1 CLI Help & Flag Audit
* **Goal:** `namegnome --help` and every sub-command list accurate, concise help entries.
* **Tests to Write:** snapshot tests of `--help` output for root, `scan`, `apply`, `undo`.
* **Steps:**
  1. Run each command, capture help, compare to expected in test fixtures.
  2. Fix missing or misleading descriptions, group flags logically.
* **Done when:** all snapshot tests pass on all OS runners.

### 0.2 Rich Progress & Error Surfaces
* **Goal:** Replace raw `print` statements with Rich spinners/progress and pretty-traceback.
* **Tests to Write:** console output tests using `rich.console.CaptureError` (or similar) verifying spinner start/stop and traceback formatting stub.
* **Steps:**
  1. Create `src/namegnome/cli/console.py` with a `ConsoleManager` context-manager that installs Rich pretty-traceback and yields a configured `Console`.
  2. Replace direct `print` and `traceback.print_exc()` calls in CLI entry-points and long-running loops with `console.print` or `console.status`.
  3. Wrap file/dir iteration loops in `rich.progress.Progress` using `SpinnerColumn` plus a custom `FilenameColumn` to show current path.
  4. Add `--no-rich` flag and `NAMEGNOME_NO_RICH` env-var; when set, fall back to plain output.
  5. Capture output in tests with `rich.console.Console.capture` and assert spinner lifecycle & coloured traceback.
  6. Update README with a before/after GIF.
* **Done when:** scan on a sample dir shows spinner; exception renders coloured traceback; tests green.

### 0.3 Shell Autocompletion
* **Goal:** Generate bash/zsh/fish completion scripts via Typer autopilot.
* **Tests to Write:** call `namegnome --show-completion bash` and assert no error / expected snippet header.
* **Steps:**
  1. Add Typer command `namegnome completion [shell]` that prints the completion script via `get_completion_script()`.
  2. Implement `namegnome init` to write the script to `$XDG_DATA_HOME/namegnome/completions` and append a `source` line to the user's shell RC file (idempotent).
  3. In CI, run the command for each supported shell and grep for the function header to ensure validity.
  4. Document manual install snippet for each shell in README.
* **Done when:** script generation works and documented.

### 0.4 Config & Env-Var Parity
* **Goal:** every CLI flag has an env-var + config-file equivalent with deterministic precedence (CLI > env > TOML config > default).
* **Tests to Write:** parametrised matrix asserting a given flag value overrides env & config; use tmp TOML file per test.
* **Steps:**
  1. Implement `utils.config.resolve_setting(key, default)` walking precedence chain.
  2. Default config file lives at `$XDG_CONFIG_HOME/namegnome/config.toml` (fallback `~/.config/namegnome/config.toml`).
  3. Add command `namegnome config docs` to render a Rich table of settings → env-var → default.
  4. Provide negative-case tests (missing key, invalid type) and precedence tests covering CLI vs ENV vs TOML.
* **Done when:** all precedence tests pass.

### 0.5 CLI Visual Rebuild
* **Goal:** Restore banner, status-gnome panels, richer progress bar, and framed summary as in `DEMO.md` screenshots.
* **Tests to Write:**
  * `tests/cli/test_visuals.py` — snapshot banner & gnome output (ANSI-stripped).
  * Param test for each gnome type produces expected emoji/message.
  * Progress bar test uses custom `FilenameColumn` to show percent · elapsed · filename.
* **Steps:**
  1. Call `print_title(console)` at start of `scan`, `apply`, `undo` (skip when `--no-color`).
  2. Add `cli/console.py` context-manager `gnome_status()`; wrap major phases (`scanning`, `planning`, `moving`, `rollback`).
  3. Implement custom Rich `FilenameColumn` & inject into all `Progress` instances.
  4. On success show `print_gnome_status("happy")`; on error show `print_gnome_status("error")`.
  5. Update `DEMO.md` with new screenshots.
* **Done when:**
  * All new tests pass; existing renderer tests unaffected.
  * Running `namegnome scan` shows banner → "Working" gnome → progress → diff → "Happy" gnome.
  * CI green on macOS/Linux/Windows.

### 0.6 Documentation Sweep
* **Goal:** Make sure that all documentation is robust and complete.
* **Steps:**)
  1. Review [README](README.md) for:
    A. Duplicate information.
    B. Incorrect legacy information (including flowcharts and diagrams).
    C. Incomplete or missing information or context.
    D. Formatting errors.
    E. Structure/flow inconsistency (make sure sections are grouped correctly and in an understandable progression).
  2. Review [/docs/](/docs/).
    A. Specifics:
      1. Delete KNOWN_ISSUES.md (this will be restored later when we've made more progress).
      2. Overhaul quickstart.md to be a better aid to first time users (this we'll retain for now, but we'll need to keep this current as we continue to add functionality).
      3. Does the project still have progress logging tools? If so, update progress-logging.md. Otherwise, delete progress-logging.md
* **Done when:** docs are polished and present the current state of the project.

---

## 2 · Sprint 1 (TV Scan Completion)

Purpose: finish outstanding TV scan logic and lock regression suite.

### 1.1 Episode-List Normalisation (RECOVERY 0.3.1)
* **Goal:** `normalize_episode_list` returns list[dict] with `season`/`episode` **int** fields; skips invalid rows.
* **Tests to Update:**
  * `tests/core/test_planner.py::TestTVPlannerHelpers::test_normalize_episode_list`
  * `tests/core/test_planner.py::test_normalize_episode_list`
* **Steps:**
  1. Accept input list[dict] from providers with keys `season`, `episode`, `title` (str/int).
  2. Coerce zero-padded strings via `int(value.lstrip('0') or 0)`.
  3. Skip rows if either field missing, non-numeric or < 0.
  4. Edge-case tests: `S00E00` (skip), `S1E05a` (skip).
* **Done when:** both tests pass with no regressions elsewhere.

### 1.2 Plan Orchestration Core Functions (RECOVERY 0.5)
* **Goal:** Implement `fetch_episode_list`, `_add_plan_item_and_callback`, `_handle_unsupported_media_type`, `_anthology_split_segments`.
* **Tests to Write:** failing unit tests in `tests/core/tv/test_plan_orchestration*.py` for each function.
* **Steps:**
  1. Define call order in `tv/plan_orchestration.py::build_plan()`:
     a. `fetch_episode_list()`
     b. `_anthology_split_segments()` when `--anthology` enabled.
     c. `_add_plan_item_and_callback()` per segment.
     d. `_handle_unsupported_media_type()` fallback.
  2. Callback must accept `(PlanItem, Console)` and may update progress.
  3. Raise `UnsupportedMediaError` for unsupported inputs; CLI converts to friendly message.
* **Done when:** all orchestration tests green.

### 1.3 Provider Fallback & Cache Invalidation
* **Goal:** if primary TV provider fails, fallback to secondary; invalidate stale cache.
* **Tests to Write:** mock provider 1 500s → provider 2 returns result.
* **Steps:**
  1. Provider priority chain: TVDB → TMDB → OMDb → AniList.
  2. Add `expires_at` field to `metadata/cache.py`; default TTL 12 h.
  3. Retry strategy: up to 3 attempts (0.5→4 s back-off) before fail-over.
  4. Mark provider unhealthy for run on repeated 5xx or quota errors.
  5. Purge cache lazily when `expires_at < now`.
* **Done when:** fallback and TTL tests green.

### 1.4 CLI TV Integration Happy Path
* **Goal:** `namegnome scan --media-type tv` on fixture tree exits 0 and emits valid plan JSON.
* **Tests to Write:** subprocess CLI integration test using tmpdir.
* **Steps:**
  1. Use test fixture library `tests/mocks/tv/...` as input.
  2. Run `namegnome scan --media-type tv --json -o plan.json` inside tmpdir.
  3. Parse `plan.json`, assert required keys (`items`, `root_dir`).
  4. Ensure exit code 0 and no stderr.
* **Done when:** integration test passes in CI.

### 1.5 Documentation Sweep
* **Goal:** Make sure that all documentation is robust and complete.
* **Steps:**)
  1. Review [README](README.md) for:
    A. Duplicate information.
    B. Incorrect legacy information (including flowcharts and diagrams).
    C. Incomplete or missing information or context.
    D. Formatting errors.
    E. Structure/flow inconsistency (make sure sections are grouped correctly and in an understandable progression).
  2. Review [/docs/](/docs/).
    A. Specifics:
      1. Delete KNOWN_ISSUES.md (this will be restored later when we've made more progress).
      2. Overhaul quickstart.md to be a better aid to first time users (this we'll retain for now, but we'll need to keep this current as we continue to add functionality).
      3. Does the project still have progress logging tools? If so, update progress-logging.md. Otherwise, delete progress-logging.md
* **Done when:** docs are polished and present the current state of the project.

### 1.6 Regression Test Suite — End-to-End TV Scan Flow
* **Goal:** Establish a rock-solid regression harness that exercises the *entire* TV scan pipeline (CLI + library) across real-world edge-case filenames, provider-failure scenarios, and LLM confidence paths.

* **Overall success criteria:**
  * Suite passes on macOS, Linux, and Windows runners.
  * Coverage for the scan pathway ≥ 85 %.
  * Runtime of happy-path test ≤ 5 s (network calls mocked).
  * Any change that breaks expected scan flow fails CI immediately.

* **Atomic sub-tasks:**

  | ID | Scope | Outcome |
  |----|-------|---------|
  | **1.6-a** | **Fixture manifest** | Add `fixture_manifest.yaml` enumerating every file in `tests/mocks/tv`, its `anthology` flag, and canonical season/episode mapping. Tests will read this manifest to build param cases automatically. |
  | **1.6-b** | **Deterministic LLM stub** | Implement `tests/helpers/fake_prompt_orchestrator.py` which replaces `PromptOrchestrator` with a configurable, no-network stub. Provide a pytest fixture `stub_llm` to toggle real vs fake. |
  | **1.6-c** | **Happy-path regression** | Write `tests/integration/test_tv_scan_regression.py::test_happy_path`. Using manifest + stub (confidence 0.90) expect exit 0; compare generated `plan.json` to `expected_plan.json`. Ensure wall-clock < 5 s. |
  | **1.6-d** | **Manual-required path** | Same fixture set; run stub confidence 0.50 → expect exit 2; assert items flagged `manual`. |
  | **1.6-e** | **Provider-fallback matrix** | ✅ **COMPLETE** - Created `tests/integration/test_provider_fallback.py` with comprehensive fallback scenarios: (1) TVDB ok → exit 0/2; (2) TVDB failure → TMDB ok → exit 0/2; (3) TVDB + TMDB failure → OMDb ok → exit 0/2; (4) TVDB + TMDB + OMDb failure → AniList ok → exit 0/2; (5) all providers fail → items flagged manual → exit 1/2. Extended OMDb client to full MetadataClient. Added `ExitCode.UNSUPPORTED = 3` to CLI. All 6 tests pass. |
  | **1.6-f** | **Unsupported / malformed inputs** | ✅ **COMPLETE** - Created `tests/mocks/tv/unsupported/` with 23 test files (ignored extensions, unknown extensions, malformed names). Implemented `tests/integration/test_unsupported_inputs.py` with 7 comprehensive tests. Updated fixture manifest with unsupported file entries. Files with media extensions but unparseable names are correctly marked as `manual` items requiring user intervention. All 7 integration tests pass. |
  | **1.6-g** | **Performance guard-rail** | ✅ **COMPLETE** - Implemented `test_performance_guard_rail()` in `tests/integration/test_tv_scan_regression.py` that measures scan runtime and fails if it exceeds 5 seconds. Test passes in ~1.6s, well within the requirement. Guards against O(n²) behavior. |
  | **1.6-h** | **CI wiring** | ✅ **COMPLETE** - Added fast `integration-tv` job to CI workflow (Linux, `--no-rich` flag), created nightly cross-platform matrix workflow, and updated coverage configuration to include integration tests with 85% threshold. CI now properly tests integration regression suite. |
  | **1.6-i** | **Documentation update** | ✅ **COMPLETE** - Extended `SCAN_RULES.md` with confidence thresholds & exit-code semantics (section 4), added comprehensive "Regression Suite" section to README with local testing instructions, created `docs/fixture-manifest.md` documenting manifest format and structure. Documentation now covers all regression test aspects. |
  | **1.6-j** | **Untrusted-names span logic** | ✅ **COMPLETE** - Harvey Girls Forever! fixtures already in manifest with `title_trusted: false`. Removed `@pytest.mark.xfail` from untrusted-titles tests, fixed duration-based episode pairing logic using `runtime` field, implemented SONARR-style handling ignoring input titles, confirmed punctuation replacement working correctly. Added integration test for Harvey Girls Forever using `--untrusted-titles` and `--max-duration` flags. All 4 untrusted-titles tests pass. |
  | **1.6-k** | **Complete regression coverage** | ✅ **COMPLETE** - Added comprehensive edge case testing for 100% SCAN_RULES.md compliance: (1) Show disambiguation with multi-series mock testing and non-interactive fallback behavior, (2) Absolute path enforcement validation (already working via typer resolve_path), (3) Duration edge cases testing with anthology mode and various max-duration scenarios, (4) Provider timeout/retry behavior with graceful fallback validation. All 4 new integration tests pass. Sprint 1.6 regression suite is now fully complete. |
  | **1.6-l** | **End-to-end testing with tiered dependencies** | ✅ **COMPLETE** - Implemented comprehensive tiered end-to-end testing with graceful fallbacks for both API access and LLM availability. Created `tests/e2e/test_real_tv_scan.py` with opt-in execution via `NAMEGNOME_E2E_TESTS=1` environment variable. **Tiered approach**: (1) **Core E2E** - file operations + cached API responses (no external dependencies), (2) **API E2E** - real APIs + cached LLM responses (requires API keys), (3) **LLM E2E** - cached APIs + real LLM (requires Ollama), (4) **Full E2E** - real APIs + real LLM (requires both). Tests auto-detect available dependencies and adapt accordingly. All 8 tests perform actual file operations in tmpdir and validate: complete scan→undo cycle, perfect destination paths, series disambiguation, visual CLI elements (banner, gnomes, progress). Tests marked with `@pytest.mark.api` and `@pytest.mark.llm` skip gracefully when dependencies unavailable. Created dedicated GitHub Actions workflow `.github/workflows/e2e.yml` with Core E2E running on every PR and extended tests in nightly runs. All tests passing with comprehensive fixture coverage. |
  | **1.6-m** | **Complete anthology show E2E coverage** | ✅ **COMPLETE** - Successfully expanded E2E test coverage to all 6 shows from fixture manifest with **comprehensive dual testing strategy**. **Sampling Tests**: Created `e2e_anthology_test_files` fixture with 2-3 files per show (~14 total) for fast feedback. **Volume Tests**: Added complete `TestVolumeEndToEnd` class with **volume testing for ALL 6 hand-selected shows** to catch volume-dependent edge cases that only emerge with full directory processing. **Critical Discovery**: Volume testing validated user's concern—Paw Patrol detected mapping conflicts in 25/26 Season 1 files due to complex episode mapping, completely missed by sampling. **Comprehensive Volume Coverage**: (1) Paw Patrol - Complex mapping conflicts, (2) Harvey Girls Forever! - Untrusted names with special characters, (3) Martha Speaks - Apostrophe normalization and same-name episodes, (4) The Octonauts - Title disambiguation consistency, (5) Firebuds - Trusted title validation at scale, (6) Danger Mouse 2015 - Non-anthology control case, (7) All shows combined - Cross-show conflict detection. **Performance**: Sampling tests <30s, volume tests selective with `@pytest.mark.slow`. **Documentation**: Updated `docs/e2e-testing.md` with comprehensive volume testing approach and show-specific edge cases. **Proven Value**: Volume testing for all 6 shows ensures each hand-selected edge case is properly validated at scale, not just through sampling. |

  * **Tests to write (high level):**
  * `tests/integration/test_tv_scan_regression.py` – happy, manual, performance cases.
  * `tests/integration/test_provider_fallback.py` – provider matrix.
  * `tests/integration/test_unsupported_inputs.py` – malformed & unsupported files.
  * `tests/e2e/test_real_tv_scan.py` – end-to-end tests with real APIs and LLM.

* **Sprint 1.6-m implementation details:**
  1. **Analysis phase:**
     - Review current `e2e_test_files` fixture limitation (only Danger Mouse 2015)
     - Map each of 6 shows to their key edge cases from fixture manifest
     - Identify minimum representative sample from each show for E2E testing
  
  2. **Enhanced fixture creation:**
     - Create `e2e_anthology_test_files` fixture that samples from all 6 shows
     - Limit to 2-3 files per show for performance (total ~18 files vs current 3)
     - Include shows with different anthology characteristics:
       * **Firebuds**: trusted episode titles, standard anthology
       * **Harvey Girls Forever!**: untrusted names, punctuation issues  
       * **Martha Speaks**: anthology edge cases, complex parsing
       * **Paw Patrol**: complex episode mapping, multi-episode files
       * **The Octonauts**: special case anthology handling
       * **Danger Mouse 2015**: non-anthology control case
  
  3. **Test expansion:**
     - Add `test_anthology_shows_e2e_coverage` test using new fixture
     - Parametrize tests by show type to validate specific anthology behaviors
     - Ensure each show's unique edge cases are properly exercised
     - Validate anthology-specific CLI flags (`--anthology`, `--untrusted-titles`)
  
  4. **Performance validation:**
     - Ensure expanded test suite still completes within 30s performance limit
     - Consider parallel test execution or selective sampling if needed
     - Maintain existing tiered dependency approach (Core/API/LLM/Full)
  
  5. **Documentation:**
     - Update `docs/e2e-testing.md` to document anthology show coverage
     - Add troubleshooting guide for anthology-specific edge cases
     - Document which shows represent which edge case categories

* **Implementation notes:**
  1. **Manifest row format:**
     ```yaml
     - file: Paw Patrol/Paw Patrol-S01E01-Pups And The Kitty Tastrophe Pups Save A Train.mp4
       anthology: true
       season: 1
       episodes: [5, 6]
       status: auto
     ```
     For Harvey Girls Forever! and similar, add:
     ```yaml
     - file: Harvey Girls Forever/Harvey Girls Forever! - S01E01 - War and Trees WEBDL-1080p.mkv
       anthology: true
       title_trusted: false
       season: 1
       episodes: [1, 2]
       status: auto
     ```
  2. **Expected plan JSON** must include keys: `src`, `dst`, `status` (auto/manual/unsupported).
  3. **Confidence thresholds:** ≥ 0.75 → auto, 0.40–0.74 → manual, < 0.40 → unsupported.
  4. **Helper fixtures** should copy only required test files into a tmpdir for speed.
  5. **End-to-end test requirements:**
     - Opt-in execution: tests only run when `NAMEGNOME_E2E_TESTS=1` is set
     - **Tiered dependency approach** with automatic detection and graceful fallbacks:
       * **Core E2E** (no external deps): cached API responses + deterministic LLM + file ops
       * **API E2E** (requires API keys): real APIs + deterministic LLM + file ops  
       * **LLM E2E** (requires Ollama): cached APIs + real LLM + file ops
       * **Full E2E** (requires both): real APIs + real LLM + file ops
     - **Test markers**: `@pytest.mark.api` (requires API keys), `@pytest.mark.llm` (requires Ollama)
     - **Cached API responses**: store real API responses in `tests/e2e/fixtures/` for dependency-free testing
     - Test isolation: each test uses isolated tmpdir for file operations
     - Graceful failure: tests skip with clear message if dependencies unavailable
     - Performance validation: end-to-end scan should complete < 30s for real data
     - Complete cycle testing: scan → apply → undo → verify original state restored
     - **CI compatibility**: Core E2E tests run by default (no API keys or LLM required)

* **Done when:**
  * All sub-tasks **1.6-a** → **1.6-m** are merged and green in CI.
  * Regression harness prevents future PRs from regressing TV scan behaviour.
  * End-to-end tests validate complete pipeline with real external dependencies.
  * Anthology show E2E coverage ensures all 6 fixture shows are properly tested.

---

### Sprint 1.7 (TV Pipeline Bug Fixes)

Purpose: Fix fundamental TV scan pipeline issues uncovered by comprehensive volume testing.

### Context: Volume Testing Discoveries

The comprehensive volume testing in Sprint 1.6-m successfully identified critical pipeline failures:
- **0% auto-success rate** across all 6 hand-selected shows
- **Filename parsing failures**: `Paw Patrol-S01E01-Title.mp4` → `season: null, episode: null`
- **Show recognition failures**: All shows processed as "Unknown Show"
- **Provider integration issues**: Mocked providers not working in E2E scenarios
- **LLM fallback chain failures**: "No confident match after LLM/manual fallback"
- **Destination conflicts**: Multiple files mapping to same generic path

### 1.7.1 Filename Parsing Core Fix
* **Goal:** Fix episode parser to correctly extract show/season/episode from standard TV filenames.
* **Tests to Write:** 
  * `tests/core/test_episode_parser_hotspot.py::test_standard_tv_filename_patterns`
  * Test patterns: `Show Name-S01E01-Episode Title.ext`, `Show-S01E01E02-Multi Episode.ext`
* **Steps:**
  1. Debug `core/episode_parser.py` regex patterns for TV filename extraction.
  2. Ensure patterns match our test fixture naming conventions exactly.
  3. Add specific test cases for each of the 6 hand-selected show naming patterns.
  4. Validate season/episode extraction returns integers, not null.
* **Done when:** `episode_parser.parse_filename("Paw Patrol-S01E01-Title.mp4")` returns `season=1, episode=1`.

### 1.7.2 Show Recognition & Metadata Integration
* **Goal:** Fix show name recognition and metadata provider integration in E2E testing scenarios.
* **Tests to Write:**
  * `tests/integration/test_show_recognition_hotspot.py`
  * Mock provider tests returning expected metadata for fixture shows.
* **Steps:**
  1. Debug why show names like "Paw Patrol" aren't being recognized from filenames.
  2. Fix metadata provider integration to use mocked responses properly in E2E tests.
  3. Ensure show name normalization ("The Octonauts" vs "Octonauts") works correctly.
  4. Add confidence scoring for show name matches.
* **Done when:** E2E scan recognizes "Paw Patrol" from filename and returns proper show metadata.

### 1.7.3 LLM Fallback Chain Repair
* **Goal:** Fix LLM confidence chain and manual fallback logic to prevent "no confident match" failures.
* **Tests to Write:**
  * `tests/llm/test_confidence_chain_hotspot.py`
  * Test confidence thresholds: ≥0.75 → auto, 0.40-0.74 → manual, <0.40 → unsupported.
* **Steps:**
  1. Debug LLM prompt orchestrator confidence scoring in E2E scenarios.
  2. Fix deterministic LLM mock to return appropriate confidence scores.
  3. Ensure fallback chain: provider → LLM → manual works correctly.
  4. Add logging for confidence decision points.
* **Done when:** LLM returns confidence scores and fallback logic routes items correctly.

### 1.7.4 Anthology Episode Splitting Logic
* **Goal:** Fix anthology mode episode splitting for multi-episode files.
* **Tests to Write:**
  * `tests/core/tv/test_anthology_splitting_hotspot.py`
  * Test `Pup Pup Boogie Pups In A Fog.mp4` → splits into 2 episodes.
* **Steps:**
  1. Debug anthology segment splitter for filenames with multiple episode titles.
  2. Fix duration-based episode pairing logic (`--max-duration` flag).
  3. Ensure proper episode numbering for split segments.
  4. Add validation for episode title extraction from multi-title filenames.
* **Done when:** `--anthology` correctly splits multi-episode files and assigns proper episode numbers.

### 1.7.5 E2E Mock Provider Integration
* **Goal:** Fix mocked provider responses in E2E testing to return realistic metadata.
* **Tests to Write:**
  * `tests/e2e/test_mock_provider_integration.py`
  * Validate mocked providers return expected metadata for each fixture show.
* **Steps:**
  1. Fix `mock_api_providers` fixture to return proper show metadata for our 6 test shows.
  2. Ensure cached API responses in `tests/e2e/fixtures/` are properly loaded.
  3. Add provider health checks in E2E scenarios.
  4. Validate metadata consistency across provider fallback chain.
* **Done when:** E2E tests use mocked providers that return realistic metadata for fixture shows.

### 1.7.6 Volume Test Validation
* **Goal:** Re-run volume tests to validate fixes and achieve reasonable auto-success rates.
* **Tests to Update:** All volume tests in `TestVolumeEndToEnd` class.
* **Steps:**
  1. Re-run all 7 volume tests after implementing fixes.
  2. Target auto-success rates: Paw Patrol ≥50%, other shows ≥70%.
  3. Validate conflict reduction: should see significantly fewer destination conflicts.
  4. Document before/after comparison of volume test results.
* **Done when:** Volume tests show substantial improvement in auto-success rates and conflict reduction.

### 1.7.7 Integration Test Recovery
* **Goal:** Ensure all integration tests pass after pipeline fixes.
* **Tests to Validate:** 
  * All tests in `tests/integration/test_tv_scan_regression.py`
  * All tests in `tests/integration/test_provider_fallback.py`
* **Steps:**
  1. Run full integration test suite after implementing fixes.
  2. Fix any regressions introduced by pipeline changes.
  3. Ensure provider fallback behavior still works correctly.
  4. Validate performance requirements still met (≤5s for happy path).
* **Done when:** All integration tests pass with improved pipeline functionality.

### 1.7.8 Documentation Update
* **Goal:** Document pipeline fixes and updated volume test results.
* **Steps:**
  1. Update `SCAN_RULES.md` with any parsing rule changes.
  2. Update `docs/e2e-testing.md` with volume test improvements.
  3. Add troubleshooting guide for filename parsing issues.
  4. Document confidence threshold behavior and fallback chain.
* **Done when:** Documentation reflects fixed pipeline behavior and improved volume test results.

* **Sprint 1.7 success criteria:**
  * Volume tests show ≥50% auto-success rate for complex shows (Paw Patrol).
  * Volume tests show ≥70% auto-success rate for standard shows.
  * All integration tests pass.
  * Core filename parsing works for all 6 fixture show patterns.
  * Provider fallback chain functions correctly in E2E scenarios.

---

## 3 · Sprint 2 (Movie Scan MVP)

Purpose: extend pipeline to handle movies end-to-end with minimal provider data.

### 2.1 Movie Planner Stub
* **Goal:** create `MovieRuleSet` and planner path logic.
* **Tests to Write:** given sample `Inception (2010).mkv` returns expected path.
* **Steps:**
  1. Target path format: `<root>/Movies/<Title> (<Year>)/<Title> (<Year>).ext`.
  2. Multi-part releases (`CD1`, `Part 02`) → suffix "– Part NN".
  3. Extras prefixed `extra-` → move to `Extras/` sub-folder.
  4. Implement `core/movie_planner.py` with `MovieRuleSet` mirroring TV rule interface.
* **Done when:** rule tests green.

### 2.2 TMDB Movie Provider Hook
* **Goal:** fetch basic metadata (title, year) from TMDB or stub when offline.
* **Tests to Write:** provider mocked call mapping.
* **Steps:**
  1. Require env `TMDB_API_KEY`; fail fast with helpful message if missing.
  2. Respect `X-RateLimit-Remaining`; if 0, sleep until `X-RateLimit-Reset`.
  3. Supply offline JSON fixtures under `tests/metadata/fixtures/tmdb/` activated by `--offline` flag.
* **Done when:** provider tests pass.

### 2.3 CLI Movie Integration Test
* **Goal:** scan sample movie dir → plan JSON.
* **Tests:** subprocess integration.
* **Steps:**
  1. Use fixture dir with `Inception (2010).mkv`.
  2. Run `namegnome scan --media-type movie -o plan.json`.
  3. Validate JSON schema and exit code.
* **Done when:** test passes and coverage ≥80 %.

### 2.4 Documentation Sweep
* **Goal:** Make sure that all documentation is robust and complete.
* **Steps:**)
  1. Review [README](README.md) for:
    A. Duplicate information.
    B. Incorrect legacy information (including flowcharts and diagrams).
    C. Incomplete or missing information or context.
    D. Formatting errors.
    E. Structure/flow inconsistency (make sure sections are grouped correctly and in an understandable progression).
  2. Review [/docs/](/docs/).
    A. Specifics:
      1. Delete KNOWN_ISSUES.md (this will be restored later when we've made more progress).
      2. Overhaul quickstart.md to be a better aid to first time users (this we'll retain for now, but we'll need to keep this current as we continue to add functionality).
      3. Does the project still have progress logging tools? If so, update progress-logging.md. Otherwise, delete progress-logging.md
* **Done when:** docs are polished and present the current state of the project.

---

## 4 · Sprint 3 (Music Scan MVP)

Purpose: add basic album/track rename pipeline.

### 3.1 Directory Heuristics & Tag Reader
* **Goal:** detect artist/album/track from hierarchy or ID3 tags (mutagen).
* **Tests:** fixture MP3 with tags vs bare filename fallback.
* **Steps:**
  1. Supported extensions: mp3, flac, m4a, ogg, opus, wav.
  2. Precedence: ID3 tags → regex parse `<Artist>/<Album>/<TrackNo> - <Title>` → filename fallback.
  3. Implement helper in `core/music/tag_reader.py`; use `mutagen` (add to `requirements.txt`).
  4. Handle various-artists and disc numbers (`Disc # – Track #`).
* **Done when:** detection tests pass.

### 3.2 Music Planner
* **Goal:** format path `Artist/Album/## ‑ Title.ext`.
* **Tests:** sample file yields expected target.
* **Steps:**
  1. Use info from previous heuristic to build `Path`.
  2. Zero-pad track numbers to 2 digits; preserve extension.
  3. Sanitize characters not allowed on Windows.
* **Done when:** planner tests green.

### 3.3 CLI Music Integration Test
* **Goal:** end-to-end scan sample music dir exits 0.
* **Tests:** subprocess integration.
* **Steps:**
  1. Fixture directory with mixed tagged/untagged files.
  2. Run `namegnome scan --media-type music -o plan.json` and assert valid output.
* **Done when:** test green.

### 3.4 Documentation Sweep
* **Goal:** Make sure that all documentation is robust and complete.
* **Steps:**)
  1. Review [README](README.md) for:
    A. Duplicate information.
    B. Incorrect legacy information (including flowcharts and diagrams).
    C. Incomplete or missing information or context.
    D. Formatting errors.
    E. Structure/flow inconsistency (make sure sections are grouped correctly and in an understandable progression).
  2. Review [/docs/](/docs/).
    A. Specifics:
      1. Delete KNOWN_ISSUES.md (this will be restored later when we've made more progress).
      2. Overhaul quickstart.md to be a better aid to first time users (this we'll retain for now, but we'll need to keep this current as we continue to add functionality).
      3. Does the project still have progress logging tools? If so, update progress-logging.md. Otherwise, delete progress-logging.md
* **Done when:** docs are polished and present the current state of the project.

---

## 5 · Sprint 4 (Apply Engine)

Purpose: safely execute rename plans with rollback.

### 4.1 `atomic_move` Implementation (see TASK 1.1 for spec)
* **Tests to Write:** same list as original TASK.md 1.1.
* **Steps:**
  1. POSIX: attempt `os.rename()`; if cross-device, stream copy to temp then replace.
  2. Windows: call `MoveFileExW` with `MOVEFILE_REPLACE_EXISTING | MOVEFILE_WRITE_THROUGH`.
  3. Add optional `--verify` flag to compare SHA-256 checksums pre/post move.
  4. Write rollback journal (JSONL) and restore on failure.
* **Done when:** all FS op tests pass.

### 4.2 `apply` CLI Command
* **Goal:** transactional apply with progress bar.
* **Tests:** success path, mid-failure rollback, dry-run.
* **Steps:**
  1. Load plan JSON, validate MD5 checksum.
  2. If `--dry-run`, print rich diff table, exit 0.
  3. Wrap execution in `ConsoleManager`; update progress per file.
  4. On exception or SIGINT, trigger rollback via journal.
* **Done when:** command works & tests pass.

### 4.3 Plan Store Housekeeping
* **Goal:** Provide CLI & library utilities to prune and query stored plans (delete all, keep-last-N, cap per directory, fetch latest plan ID / print latest plan).
* **Tests to Write:**
  * `tests/utils/test_plan_store_housekeeping.py` covering:
    * `delete_all_plans()` removes all JSON & meta files.
    * `delete_all_but_latest(n)` keeps latest *n* by timestamp.
    * `get_latest_plan_id_by_root(path)` returns newest plan generated for that root dir.
    * `print_latest_plan(path)` pretty-prints plan table to console.
    * Retention cap: saving the 11th plan for same root auto-deletes oldest so only 10 remain.
* **Steps:**
  1. Extend `utils.plan_store` with:
     ```python
     def delete_plans(keep_last: int | None = None) -> int
     def get_latest_plan_id_by_root(root: Path) -> str | None
     def print_latest_plan(root: Path, console: Console | None = None) -> None
     ```
  2. Record `root_dir` in `RunMetadata.args` when saving plans (already present as `scan_options.root_dir` – expose helper to filter).
  3. Add Typer sub-command group `plan` with:
     * `plan list`  – list stored plan IDs with timestamp & root.
     * `plan prune --keep N` – delete all but last *N* plans globally or per-root (`--root PATH`).
     * `plan latest --root PATH` – print latest ID.
     * `plan show --root PATH` – pretty-print latest plan diff.
  4. Enforce automatic cap of 10 plans per root in `save_plan()`; when exceeded, delete oldest plan+meta.
  5. Update docs (`README`, `DEMO.md`) to reference new commands and retention policy.
* **Done when:**
  * All housekeeping tests pass, including automatic cap.
  * New CLI commands work in integration test and docs.
  * `save_plan()` auto-prunes to ≤10 plans per root.

### 4.4 Documentation Sweep
* **Goal:** Make sure that all documentation is robust and complete.
* **Steps:**)
  1. Review [README](README.md) for:
    A. Duplicate information.
    B. Incorrect legacy information (including flowcharts and diagrams).
    C. Incomplete or missing information or context.
    D. Formatting errors.
    E. Structure/flow inconsistency (make sure sections are grouped correctly and in an understandable progression).
  2. Review [/docs/](/docs/).
    A. Specifics:
      1. Delete KNOWN_ISSUES.md (this will be restored later when we've made more progress).
      2. Overhaul quickstart.md to be a better aid to first time users (this we'll retain for now, but we'll need to keep this current as we continue to add functionality).
      3. Does the project still have progress logging tools? If so, update progress-logging.md. Otherwise, delete progress-logging.md
* **Done when:** docs are polished and present the current state of the project.

---

## 6 · Sprint 5 (Undo Engine)

### 5.1 `undo` Core Logic
* **Goal:** revert a completed plan safely.
* **Tests:** round-trip integration (scan → apply → undo) on tmpdir.
* **Steps:**
  1. Generate undo plan alongside apply journal with src/dst swapped.
  2. Execute moves in reverse order under lock file (`undo.lock`).
  3. Verify post-undo state matches original via checksum.
* **Done when:** library state identical pre/post.

### 5.2 CLI UX & Listing
* **Goal:** `namegnome undo --list` displays stored plans.
* **Tests:** snapshot of table output.
* **Steps:**
  1. Implement `undo list` Typer subcommand printing table with Plan ID, root, created, status.
  2. Add `--all` flag to bypass pagination (default 10 rows).
* **Done when:** list tests pass.

### 5.3 Documentation Sweep
* **Goal:** Make sure that all documentation is robust and complete.
* **Steps:**)
  1. Review [README](README.md) for:
    A. Duplicate information.
    B. Incorrect legacy information (including flowcharts and diagrams).
    C. Incomplete or missing information or context.
    D. Formatting errors.
    E. Structure/flow inconsistency (make sure sections are grouped correctly and in an understandable progression).
  2. Review [/docs/](/docs/).
    A. Specifics:
      1. Delete KNOWN_ISSUES.md (this will be restored later when we've made more progress).
      2. Overhaul quickstart.md to be a better aid to first time users (this we'll retain for now, but we'll need to keep this current as we continue to add functionality).
      3. Does the project still have progress logging tools? If so, update progress-logging.md. Otherwise, delete progress-logging.md
* **Done when:** docs are polished and present the current state of the project.

---

## 7 · Sprint 6 (GUI Tech Spike)

Purpose: evaluate frameworks and ship minimal viewer prototype.

### 6.1 Framework Selection Spike
* **Goal:** produce comparison doc (Textual vs Toga vs Electron vs Web) with PoC branch.
* **Tests:** N/A – spike, time-boxed.
* **Steps:**
  1. Create matrix comparing dev experience, cross-platform support, bundle size, theming.
  2. Prototype "Hello-world" in top two contenders; record build instructions.
  3. Document decision in `docs/gui-framework-decision.md`.
* **Done when:** decision documented & PoC archived.

### 6.2 Minimal Scan Viewer
* **Goal:** list scan plans and allow approve/apply.
* **Tests:** e2e Playwright happy path clicking approve triggers `apply` API call (mocked).
* **Steps:**
  1. Backend: lightweight FastAPI exposing `/plans`, `/apply/{id}`; bind to 127.0.0.1 with token auth.
  2. Front-end: Textual-web app fetches `/plans` via WebSocket, renders table, shows approve button.
  3. SSE stream `/apply/{id}/progress` updates progress bar.
* **Done when:** viewer PoC demo recorded.

### 6.3 Documentation Sweep
* **Goal:** Make sure that all documentation is robust and complete.
* **Steps:**)
  1. Review [README](README.md) for:
    A. Duplicate information.
    B. Incorrect legacy information (including flowcharts and diagrams).
    C. Incomplete or missing information or context.
    D. Formatting errors.
    E. Structure/flow inconsistency (make sure sections are grouped correctly and in an understandable progression).
  2. Review [/docs/](/docs/).
    A. Specifics:
      1. Delete KNOWN_ISSUES.md (this will be restored later when we've made more progress).
      2. Overhaul quickstart.md to be a better aid to first time users (this we'll retain for now, but we'll need to keep this current as we continue to add functionality).
      3. Does the project still have progress logging tools? If so, update progress-logging.md. Otherwise, delete progress-logging.md
* **Done when:** docs are polished and present the current state of the project.

---

## 8 · Cross-Cutting Epics (Ongoing)

* **Provider Rate-Limit Handling** – retry with exponential backoff; shared util tests.
* **Windows Path Edge-Cases** – path length, reserved names; targeted tests.
* **Performance Profiling** – 10k file library benchmark, create ticket if >30 s.
* **Security & Dependency Audit** – monthly Dependabot review.

---

> **How to use:**  
> Pick the next ticket, create a branch `topic/<ticket-slug>`, drive it with a failing test, commit, PR, and mark as ✅ here when merged. 